\section{Vector Spaces}

\subsection{Introduction to Vector Spaces}

\begin{definition}[Vector Space]
	A \textit{vector space} $V$ over a field $\FF$ is a set with two binary operations, $+:V\times V\rightarrow V$ and $\cdot:V\times\FF\rightarrow V$ such that all of the following hold.
	\begin{enumerate}
		\item For all $x,y\in V$, $x+y=y+x$. (Additive Commutativity)
		\item For all $x,y,z\in V$, $x+(y+z)=(x+y)+z$. (Additive Associativity)
		\item There exists an element, denoted 0, in $V$ such that for all $x\in V$, $x+0=x$.
		\item For each $x\in V$ there exists a $y\in V$, denoted $-x$, such that $x+y=0$.
		\item For all $x\in V$, $1x=x$.
		\item For all $a,b\in\FF$ and $x\in V$, $a(bx)=(ab)x$.
		\item For all $a\in\FF$ and $x,y\in V$, $a(x+y)=ax+ay$.
		\item For all $a,b\in\FF$ and $x\in V$, $(a+b)x=ax+bx$.
	\end{enumerate}
	Furthermore, $x+y$ is called the \textit{sum of $x$ and $y$} while $ax$ is called the \textit{product of $x$ and $a$}.
	Moreover, each $x\in V$ is called a \textit{vector} and each $a\in\FF$ is called a \textit{scalar}.
\end{definition}

\begin{definition}[$n$-tuple]
	An object of the form $(a_1,a_2,\ldots,a_n)$ where $a_j\in\FF$ for all $1\leq j\leq n$, is called an \textit{$n$-tuple}.
\end{definition}

\begin{example}
	Let $\FF$ be a field and $n\in\NN$, then $\FF^n=\set{(a_1,a_2,\ldots,a_n)| a_j\in\FF \forall 1\leq j\leq n}$ forms a vector space under component-wise addition and multiplication as defined below for $(a_1,a_2,\ldots ,a_n),(b_1,b_2,\ldots,b_n)\in\FF^n$ and $k\in\FF$.
	\begin{align*}
		(a_1,a_2,\ldots ,a_n)+(b_1,b_2,\ldots,b_n)&=(a_1+b_1,a_2+b_2,\ldots,a_n+b_n)\\
		k(a_1,a_2,\ldots ,a_n)&=(ka_1,ka_2,\ldots,ka_n)
	\end{align*}
	Furthermore, it said that
	\[
		(a_1,a_2,\ldots ,a_n)=(b_1,b_2,\ldots,b_n)
	\]
	if and only if $a_j=b_j$ for all $1\leq j\leq n$.
\end{example}
\begin{proof}
	$\FF^n$ is a vector space trivially from the fact that $\FF$ is a field.
\end{proof}

\begin{definition}[Matrix]
	Let $\FF$ be a field and $m,n\in\NN$, then an \textit{$m\times n$ matrix} with entries from $\FF$ is a rectangular array of the form
	\[
		A=
		\begin{pmatrix}
			a_{1,1} & a_{1,2} & \ldots & a_{1,n}\\
			a_{2,1} & a_{2,2} & \ldots & a_{2,n}\\
			\vdots & \vdots & \ddots & \vdots\\
			a_{m,1} & a_{m,2} & \ldots & a_{m,n}
		\end{pmatrix}
	\]
	where $a_{i,j}\in\FF$ for all $1\leq i \leq m$ and $1\leq j \leq n$.
	The entries $(a_{i,1},a_{i,2},\ldots,a_{i,n})$ is called the \textit{$i$th row} of the matrix and is a row vector in $\FF^n$.
	The entries $(a_{1,j},a_{2,j},\ldots,a_{m,j})$ is called the \textit{$j$th column} of the matrix and is a column vector in $\FF^n$.
	We denote the entry on the $i$th row and $j$th column as $A_{i,j}$.
	Furthermore, two $m\times n$ matrices, $A$ and $B$, are equal if and only if $A_{i,j}=B_{i,j}$ for all $1\leq i\leq m$ and $1\leq j\leq n$; we denote this by $A=B$.
	Moreover, if $n=m$ we say that $A$ is a \textit{square matrix}.
	Lastly, we denote the set of $m\times n$ matrices over $\FF$ as $M_{m\times n}(\FF)$.
\end{definition}

\begin{example}
	Let $\FF$ be a field and $m,n\in\NN$, then $M_{m\times n}(\FF)$ is a vector space over $\FF$ under the following operations for $A,B\in M_{m\times n}(\FF)$ and $k\in\FF$.
	\begin{align*}
		(A+B)_{i,j}&=A_{i,j}+B_{i,j}\\
		(kA)_{i,j}&=kA_{i,j}
	\end{align*}
\end{example}
\begin{proof}
	The proof is trivial from the fact that we operating on multiple copies of a field.
\end{proof}

\begin{example}
	Let $S$ be a nonempty set and let $\FF$ be a field and let $\mathscr{F}(S,\FF)$ denote the set of all functions from $S$ into $\FF$.
	Two elements $f,g\in\mathscr{F}(S,\FF)$ are equal if and only if $f(s)=g(s)$ for all $s\in S$.
	Then $\mathscr{F}(S,\FF)$ is a vector space under the following operations for $f,g\in\mathscr{F}(S,\FF)$ and $k\in\FF$.
	\begin{align*}
		(f+g)(s)&=f(s)+g(s)\\
		(kf)(s)&=k\brac{f(s)}
	\end{align*}
\end{example}
\begin{proof}
	The proof is trivial because all operations are done inside the field, and thus the space inherits the structure from $\FF$.
\end{proof}

\begin{definition}[Polynomial Ring]
	Let $\FF$ be a field.
	Then the \textit{ring of polynomials in an indeterminate $x$ over $\FF$}, denoted $\FF[x]$ is defined as
	\[
		\FF[x] := \set{\sum_{i=0}^n a_i x^i | n\in\NN, (a_0,a_1,\ldots,a_n)\in\FF^n, a_n\neq 0}.
	\]
	Additionally, we define $x^0=1$.
	Moreover, for each $p=\sum_{i=0}^n p_ix^i\in\FF[x]$, the \textit{degree of $p$}, denoted $\deg p$, is $n$.
	Furthermore, if $p=0$, that is $p_n=p_{n-1}=\ldots=p_0=0$, then $p$ is called the \textit{zero polynomial} and $\deg p = -1$ or $\deg p =-\infty$ depending on convention.
	If $\deg p=0$, then we say $p$ is a \textit{constant polynomial}.
	Lastly, $\FF[x]$ forms a ring under the following operations where $p,q\in\FF[x]$ and without loss of generality assume, $\deg p \geq\deg q$.
	\begin{align*}
		p+q &=\sum_{i=0}^{\deg q} (p_i+q_i)x^i +\sum_{i=\deg q + 1}^{\deg p} p_ix^i\\
		p q &= \sum_{k=0}^{\deg p \cdot \deg q} \paren{\sum_{i+j=k} p_iq_j} x^k
	\end{align*}
\end{definition}
\begin{example}
	Let $\FF$ be a field, then $\FF[x]$ is a vector space over $\FF$ under polynomial addition and scalar multiplication by constant polynomials.
\end{example}
\begin{proof}
	Since $\FF[x]$ is a ring, it is closed under addition.
	Since constant polynomials are polynomials, it is closed under scalar multiplication.
	Since $\FF[x]$ is a ring, addition is commutative and associative.
	Moreover the zero polynomial is the additive identity and likewise serves as the zero vector.
	Since $\FF[x]$ is a ring, each $p\in\FF[x]$ has a unique $-p\in\FF[x]$ such that $p+(-p)=0$.
	The constant polynomial $1$ is the multiplicative identity in the ring and serves as the identity in the vector space.
	Furthermore, multiplication is associative and distributes over addition because $\FF[x]$ is, indeed, a ring.
\end{proof}

\begin{proposition}
	If $u,v,w$ are elements of a vector space $V$ such that $x+z=y+z$ then, $x=y$.
\end{proposition}
\begin{proof}
	Since $z\in V$, then there exists a $-z\in V$ such that $z+(-z)=0$.
	Thus, $x+z=y+z$ implies
	\[
		x+z-z=y+z-z
	\]
	and ergo $x=y$.
\end{proof}

\begin{proposition}
	The zero vector in any vector space $V$ is unique.
\end{proposition}
\begin{proof}
	Assume there exists two zero vectors in $V$ denoted $0_1$ and $0_2$.
	Then $v+0_1=v$ and $v+0_2=v$ for all $v\in V$.
	Therefore it is true that
	\[
		0_2 = 0_2+0_1 = 0_1 + 0_2 = 0_1
	\]
	and thus these identities are in fact the same.
\end{proof}
\begin{proposition}
	Let $V$ be a vector space and let $v\in V$, then there exists a unique $u\in V$ such that $v+u=0$.
\end{proposition}
\begin{proof}
	Assume $v$ has two inverses, namely $u_1$ and $u_2$.
	Then $v+u_1=0$ and $v+u_2=0$.
	Therefore,
	\[
		v+u_1+u_2=0+u_2=u_2
	\]
	and
	\[
		v+u_1+u_2=v+u_2+u_1=0+u_1=u_1.
	\]
	Ergo, $u_1=u_2$ and the inverse of $v$ is unique.
\end{proof}
\begin{proposition}
	Let $V$ be a vector space over a field $\FF$, then:
	\begin{enumerate}
		\item $0x=0$ for all $x\in V$;
		\item $a0 = 0$ for all $a\in\FF$;
		\item $(-a)x=-(ax)=a(-x)$ for all $x\in V$ and $a\in\FF$.
	\end{enumerate}
\end{proposition}
\begin{proof}[Proof (1):]
	Consider $0x+0x$.
	Then,
	\[
		0x+0x=(0+0)x=0x=0+0x.
	\]
	Since $0x+0x=0+0x$, by cancellation, we have $0x=0$.
\end{proof}
\begin{proof}[Proof (2)]
	Consider $a0+a0$.
	\[
		a0+a0=a(0+0)=a0=a0+0
	\]
	Thus, by cancellation, $a0=0$.
\end{proof}
\begin{proof}[Proof (3)]
	Consider $-(ax)$.
	We know that $-(ax)$ is the unique additive inverse of $ax$, thus it is enough to show that $(-a)x$ and $a(-x)$ are inverses of $ax$.
	\begin{align*}
		ax+(-a)x &= (a-a)x\\
		&= 0x\\
		&= 0\\
		ax+a(-x)&= a(x-x)\\
		&=a0\\
		&=0
	\end{align*}
	Thus $a(-x)$ and $(-a)x$ are inverses of $ax$ and $(-a)x=-(ax)=a(-x)$.
\end{proof}

\pagebreak

\subsection{Subspaces}

\begin{definition}[Subspace]
	A \textit{subspace}, $W$, of a vector space, $V$, over a field, $\FF$, is a subset of $V$ that is also a vector space over $\FF$.
\end{definition}
\begin{example}
	For any vector space $V$, $V$ and $\set{0}$ are subspaces of $V$.
	The latter is called the \textit{zero subspace}.
\end{example}

\begin{thm}
	Let $V$ be a vector space over a field $\FF$ and let $W\subseteq V$.
	Then $W$ is a subspace of $V$ if and only if all of the following are satisfied.
	\begin{itemize}
		\item $0\in W$.
		\item For all $x,y\in W$, $x+y\in W$.
		\item For all $a\in\FF$ and $x\in W$, $ax\in W$.
	\end{itemize}
\end{thm}
\begin{proof}
	$\Rightarrow$ Since $W$ is a subspace of $V$, $0\in W$ and $W$ is closed under $V$'s vector addition and scalar multiplication.

	$\Leftarrow$ Since $V$ is a vector space $W$ inherits associativity, commutativity, and distributivity from $V$ as well as $V$'s behavior with respect to the identities.
	Furthermore, $0\in W$ by assumption.
	All that is left to show is that $W$ contains additive inverses.
	Suppose $x\in W$, then by assumption $-x=(-1)x\in W$.
	Thus $W$ is a subspace of $V$.
\end{proof}

\begin{definition}[Matrix Transpose]
	Let $M$ be an $m\times n$ matrix, then the \textit{transpose of $M$}, denoted $M^T$, is the $n\times m$ matrix defined by $(M^T)_{i,j}=M_{j,i}$, that is
	\[
		M^T=
		\begin{pmatrix}
			M_{1,1} & M_{2,1} & \ldots & M_{m,1}\\
			M_{1,2} & M_{2,2} & \ldots & M_{m,2}\\
			\vdots & \vdots & \ddots & \vdots\\
			M_{1,n} & M_{2,n} & \ldots & M_{m,n}
		\end{pmatrix}.
	\]
\end{definition}

\begin{definition}[Symmetric Matrix]
	Let $M$ be a matrix, then if $M=M^T$, we say $M$ is \textit{symmetric}.
\end{definition}

\begin{example}
	The set of symmetric $n\times n$ matrices over a field $\FF$, denoted $W_{n\times n}(\FF)$, is a subspace of $M_{n\times n}(\FF)$.
\end{example}
\begin{proof}
	Consider the zero matrix.
	Since the zero matrix is an $n\times n$ matrix with all entries equal to zero, the transpose of the zero matrix is also an $n\times n$ matrix with all entries equal to zero.
	Thus, $0=0^T$ and the zero matrix is symmetric.

	Let $A,B\in W_{n\times n}(\FF)$.
	Then $A=A^T$ and $B=B^T$.
	By definition of symmetry and matrix transpose we have
	\begin{equation}\label{subspace-eq1}
		A_{i,j}=(A^T)_{i,j}=A_{j,i}
	\end{equation}
	and
	\begin{equation}\label{subspace-eq2}
		B_{i,j}=(B^T)_{i,j}=B_{j,i}
	\end{equation}
	for all $1\leq i, j\leq n$.

	Consider $(A+B)$.
	By definition we have
	\[
		(A+B)_{i,j}=A_{i,j}+B_{i,j}.
	\]
	By \autoref{subspace-eq1} and \autoref{subspace-eq2} we have
	\[
		(A+B)_{i,j}=A_{i,j}+B_{i,j}=A_{j,i}+B_{j,i}.
	\]
	The definition of matrix transpose implies that
	\[
		(A+B)_{i,j}=A_{i,j}+B_{i,j}=A_{j,i}+B_{j,i}=(A^T)_{i,j}+(B^T)_{i,j}
	\]
	and thus by definition of matrix addition,
	\[
		(A+B)_{i,j}=A_{i,j}+B_{i,j}=A_{j,i}+B_{j,i}=(A^T)_{i,j}+(B^T)_{i,j}=(A^T+B^T)_{i,j}.
	\]
	Ergo, $A+B$ is symmetric and $W(\FF)$ is closed under matrix addition.

	Let $k\in\FF$ and consider $kA$.
	We know by definition that
	\[
		(kA)_{i,j}=k\cdot A_{i,j}.
	\]
	We invoke \autoref{subspace-eq1} again to get that
	\[
		(kA)_{i,j}=k\cdot A_{i,j}=k\cdot A_{j,i}.
	\]
	Applying the definition of matrix transpose yields,
	\[
		(kA)_{i,j}=k\cdot A_{i,j}=k\cdot A_{j,i}=k\cdot (A^T)_{i,j}.
	\]
	Lastly, by definition of scalar multiplication we have,
	\[
		(kA)_{i,j}=k\cdot A_{i,j}=k\cdot A_{j,i}=k\cdot (A^T)_{i,j}=(kA^T)_{i,j}.
	\]
	Thus $kA$ is symmetric and $W(\FF)$ is closed under scalar multiplication.
\end{proof}

\begin{definition}[Main Diagonal of a Matrix]
	Let $\FF$ be a field and let $M\in M_{n\times n}(\FF)$, then the \textit{main diagonal of $M$} is the set $\set{M_{i,i}}_{i=1}^n$.
\end{definition}

\begin{definition}[Diagonal Matrix]
	Let $\FF$ be a field and let $A\in M_{n\times n}(\FF)$, then $A$ is called a \textit{diagonal matrix} if and only if whenever $i\neq j$, $A_{i,j}=0$.
\end{definition}

\begin{example}
	Let $\FF$ be a field and let $D_n(\FF)$ be the set of all diagonal matrices in $M_{n\times n}(\FF)$, then $D_n(\FF)$ is a subspace on $M_{n\times n}(\FF)$.
\end{example}
\begin{proof}
	We know $0\in D_n(\FF)$ since for all $i,j$, $0_{i,j}=0$.
	Let $A,B\in D_n(\FF)$.
	Then for all $i\neq j$, $A_{i,j}=B_{i,j}=0$.
	Thus, $(A+B)_{i,j}=A_{i,j}+B_{i,j}=0+0=0$ whenever $i\neq j$ and $A+B$ is diagonal.
	Let $k\in\FF$.
	Then $(kA)_{i,j}=k\cdot A_{i,j}=k\cdot 0=0$ and $kA$ is diagonal.
	Therefore $D_n(\FF)$ forms a subspace of $M_{n\times n}(\FF)$.
\end{proof}

\begin{definition}[Trace of a Matrix]
	Let $\KK$ be a field and let $M\in M_{n\times n}(\KK)$, then the \textit{trace of $M$} denoted $\trace M$ is defined as
	\[
		\trace M = \sum_{i=1}^n M_{i,i}
	\]
	or the sum of the elements on the main diagonal.
\end{definition}

\begin{example}
	Let $\KK$ be a field and let $T_n(\KK)$ be the set of matrices in $M_{n\times n}(\KK)$ with trace equal to zero, then $T_n(\KK)$ is a subspace of $M_{n\times n}(\KK)$.
\end{example}
\begin{proof}
	Obviously, the zero matrix has a trace of zero and thus $0\in T_n(\KK)$.
	Let $A,B\in T_n(\KK)$ then $\trace A = 0$ and $\trace B = 0$.
	Consider $\trace (A+B)$.
	\[
		\trace (A+B)
		= \sum_{i=1}^n (A+B)_{i,i}
		= \sum_{i=1^n} (A_{i,i}+B_{i,i})
		= \paren{\sum_{i=1}^n A_{i,i}} + \paren{\sum_{i=1}^n B_{i,i}}
		= \trace A + \trace B
		= 0
	\]
	Thus, $A+B$ has trace 0 and $A+B\in T_n(\KK)$.
	Let $k\in\KK$.
	Consider $\trace (kA)$.
	\[
		\trace (kA)
		= \sum_{i=1}^n (kA)_{i,i}
		= \sum_{i=1}^n k\cdot A_{i,i}
		= k\sum_{i=1}^n A_{i,i}
		= k\trace A
		= 0
	\]
	And thus, $kA$ has trace 0 and $kA\in T_n(\KK)$.
	Therefore $T_n(\KK)$ is a subspace of $M_{n\times n}(\KK)$.
\end{proof}

\begin{thm}
	Let $V$ be a vector space over a field $\FF$ and let $\mathcal{W}$ be a countable collection of subspaces of $V$.
	Then
	\[
		W_i = \bigcap_{W\in\mathcal{W}} W
	\]
	is a subspace of $V$.
\end{thm}
\begin{proof}
	Since $0\in W$ for all $W\in\mathcal{W}$, $0\in W_i$.
	Let $x,y\in W_i$, then $x,y\in W$ for all $W\in\mathcal{W}$ and thus $x+y\in W$ for all $W\in\mathcal{W}$.
	Therefore, $x+y\in W_i$.
	Let $a\in\FF$.
	Since $x\in W$ for all $W\in\mathcal{W}$, $ax\in W$ for all $W\in\mathcal{W}$.
	Ergo, $ax\in W_i$ and $W_i$ is a subspace of $V$.
\end{proof}

\begin{proposition}
	For any matrix $A$, $[{(A^T)}^T]=A$.
\end{proposition}
\begin{proof}
	Apply the definition of matrix transposition twice.
	\[
		[{(A^T)}^T]_{i,j}=(A^T)_{j,i}=A_{i,j}
	\]
\end{proof}

\begin{proposition}
	For any matrix $A$, $A+A^T$ is symmetric.
\end{proposition}
\begin{proof}
	Consider $(A+A^T)_{i,j}$.
	\[
		(A+A^T)_{i,j} =
		A_{i,j} +(A^T)_{i,j} =
		A_{i,j} + A_{j,i} =
		A_{j,i} + A_{i,j} =
		A_{j,i} + (A^T)_{j,i} =
		(A+A^T)_{j,i} =
		[{(A+A^T)}^T]_{i,j}
	\]
	And thus, $(A+A^T)$ is symmetric.
\end{proof}

\begin{proposition}
	Let $\KK$ be a field and let $A,B\in M_{n\times n}(\KK)$ and $a,b\in\KK$, then $\trace (aA+bB)=a\trace A + b\trace B$.
\end{proposition}
\begin{proof}
	\begin{align*}
		\trace (aA+bB) &= \sum_{i=1}^n (aA+bB)_{i,i} \\
		&= \sum_{i=1}^n \brac{(aA)_{i,i}+(bB)_{i,i}} \\
		&= \paren{\sum_{i=1}^n a\cdot A_{i,i}} + \paren{\sum_{i=1}^n b\cdot B_{i,i}}\\
		&= a\paren{\sum_{i=1}^n A_{i,i}} + b\paren{\sum_{i=1}^n B_{i,i}}\\
		&= a\trace A + b\trace B
	\end{align*}
\end{proof}

\begin{definition}[Sum of Subsets of a Vector Space]
	Let $S,R$ be nonempty subsets of a vector space $V$, then the \textit{sum of $S$ and $R$}, denoted $S+R$ is defined as $S+R=\set{s+r|s\in S, r\in R}$.
\end{definition}

\begin{proposition}
	Let $U,W$ be subspaces of a vector space $V$ over a field $\FF$.
	Then $U+W$ is a subspace of $V$ and is the smallest subspace containing both $U$ and $W$.
\end{proposition}
\begin{proof}
	Since $U$ and $W$ are subspaces, $0\in U$ and $0\in W$ therefore, $0=0+0\in U+W$.
	Let $x,y\in U+W$ then there exist $u_x,u_y\in U$ and $w_x,w_y\in W$ such that $x=u_x+w_x$ and $y=u_y+w_y$.
	Thus,
	\[
		x+y=(u_x+w_x)+(u_y+w_y)=(u_x+u_y)+(w_x+w_y).
	\]
	Since $U$ and $W$ are subspaces, $u_x+u_y\in U$ and $w_x+w_y\in W$.
	Ergo, $x+y=(u_x+u_y)+(w_x+w_y)\in U+W$.

	Let $a\in\FF$.
	Then,
	\[
		ax= a(u_x+w_x)=au_x+aw_x.
	\]
	Since $U$ and $W$ are subspaces, $au_x\in U$ and $aw_x\in W$.
	Ergo, $ax=au_x+aw_x\in U+W$ and $U+W$ is a subspace of $V$.

	We know that, set-wise, $U=\set{u+0}_{u\in U}$ and $W=\set{0+w}_{w\in W}$, and thus $U,W\subseteq U+W$.
	Let $X$ be a subspace of $V$ such that $U,W\subseteq X$.
	Let $x\in U+W$, then there exists some $u\in U\subseteq X$ and $w\in W\subseteq X$ such that $x=u+w$.
	Therefore, $x=u+w\in X$ and $U+W\subseteq X$ for all subspaces $X$ containing $U$ and $W$.
	Ergo, $U+W$ is the smallest subspace of $V$ containing $U$ and $W$.
\end{proof}

\begin{definition}[Direct Sum of Vector Spaces]
	A vector space $V$ is called the \textit{direct sum of $U$ and $W$}, denoted $V=U\oplus W$ if and only if $U$ and $W$ are subspaces of $V$ such that $U\cap W=\emptyset$ and $U+W=V$.
\end{definition}

\begin{example}
	Let $\KK$ be a field and let $U=\set{(a_1,a_2,\ldots, a_n)\in\KK^n| a_n=0}$ and $V=\set{(a_1,a_2\ldots,a_n)\in\KK^n|a_1=a_2=\ldots=a_{n-1}=0}$.
	Then $\KK^n = U\oplus V$.
\end{example}
\begin{proof}
	The details are obvious and left as an exercise.
\end{proof}

\begin{definition}[Cosets of a Vector Space]
	Let $U$ be a subspace of a vector space $V$ over a field $\KK$.
	Then for each $v\in V$ the set $\set{v}+W=\set{v+w}_{w\in W}$ is called the \textit{coset of $W$ containing $v$}, denoted $v+W$.
\end{definition}

\begin{proposition}
	Let $\KK$ be a field and $W$ be a subspace of a vector space $V$ over $\FF$ and let $v\in V$.
	Then $v+W$ is a subspace if and only if $v\in W$.
\end{proposition}
\begin{proof}
	$\Leftarrow$ Suppose $v\in W$.
	Then by closure, $v+W=\set{v+w}_{w\in W}=W$.

	$\Rightarrow$ Suppose $v+W$ is a subspace of $V$.
	Then $0\in v+W$ and therefore, there exists a $w\in W$ such that $0=v+w$.
	This $w$ can only be $-v$ by uniqueness of inverses.
	Since $-v\in W$, $v\in W$ since $W$ is a subspace.
\end{proof}

\begin{proposition}
	Let $\KK$ be a field and $W$ be a subspace of a vector space $V$ over $\FF$ and let $v\in V$.
	Then $v\in v+W$.
\end{proposition}
\begin{proof}
	Since $W$ is a subspace, $0\in W$ and thus $v=v+0\in v+W$.
\end{proof}

\begin{proposition}
	Let $\KK$ be a field and $W$ be a subspace of a vector space $V$ over $\FF$ and let $u,v\in V$.
	Then $v+W\cap u+W=\emptyset$ if and only if $v+W\neq u+W$.
\end{proposition}
\begin{proof}
	$\Rightarrow$ Suppose $v+W\cap u+W=\emptyset$.
	Then since both $v+W$ and $u+W$ are non-empty, $v+W\neq u+W$.

	$\Leftarrow$ Suppose $v+W\neq u+W$ with $v+W\cap u+W\neq\emptyset$.
	Then there exists an $x\in v+W\cap u+W$.
	Ergo, $x\in v+W$ and $x\in u+W$.
	Thus, there exists $w_1,w_2\in W$ such that $x=v+w_1$ and $x=u+w_2$ respectively.
	Therefore, $v+w_1=u+w_2$ and $v=u+w_2-w_1$.
	Ergo,
	\[
		v+W=\set{v+w}_{w\in W}=\set{u+(w_2-w_1+w)}_{w\in W}=u+W.
	\]
	Thus, creating a contradiction.
	Therefore if $v+W\neq u+W$ then $v+W\cap u+W=\emptyset$.
\end{proof}

\begin{proposition}
	Let $\KK$ be a field and $W$ be a subspace of a vector space $V$ over $\FF$ and let $u,v\in V$.
	Then $v+W=u+W$ if and only if $v-u\in W$.
\end{proposition}
\begin{proof}
	$\Rightarrow$ Assume $v+W=u+W$.
	Then $v\in v+W$ and thus $v\in u+W$.
	Ergo, there exists a $w\in W$ such that $v=u+w$.
	Solving for $w$ yields $w=v-u\in W$.

	$\Leftarrow$ Assume $v-u\in W$.
	Therefore, $u+v-u=v\in u+W$.
	We know $v\in v+W$ thus, $u+W\cap v+W\neq\emptyset$.
	This occurs if and only if $u+W=v+W$.
\end{proof}

\begin{definition}[Quotient Space]
	Let $\KK$ be a field and $W$ be a subspace of a vector space $V$ over $\FF$.
	The the \textit{quotient space of $V$ modulo $W$}, denoted $V/W$ is the set of all cosets of $W$,
	\[
		V/W := \set{v+W}_{v\in V}.
	\]
	Furthermore $V/W$ is a vector space under the following operations.
	\begin{align*}
		(u+W)+(v+W)&=(u+v)+W\\
		a(u+W)&=(au)+W
	\end{align*}
\end{definition}
\begin{proof}
	A bunch of tedious symbol pushing that I refuse to do.
\end{proof}

\pagebreak

\subsection{Linear Combinations}

\begin{definition}[Linear Combination]
	Let $V$ be a vector space over a field $\FF$ and let $S$ be a nonempty subset of $V$.
	An $x\in V$ is said to be a \textit{linear combination of elements of $S$} if and only if there exists a $\set{s_j}_{j=1}^n\subseteq S$ and scalars $\set{a_j}_{j=1}^n\subseteq\FF$ where $n < \infty$ such that
	\[
		x=\sum_{j=1}^n a_j y_j.
	\]
	When this happens, we say $x$ is a \textit{linear combination of $y_1,y_2,\ldots, y_n$}.
\end{definition}

\begin{definition}[Spanning Set]
	Let $V$ be a vector space over a field $\FF$ and let $S$ be a nonempty subset of $V$.
	Then, the \textit{span of $S$}, denoted $\spann S$, is the set
	\[
		\spann S = \set{\sum_{j=1}^n a_j s_j | \set{a_j}_{j=1}^n\subseteq\FF, \set{s_j}_{j=1}^n\subseteq S, n<\infty}
	\]
	or the set of linear combinations of elements of $S$.
	We define $\spann\emptyset = \set{0}$.
\end{definition}

\begin{thm}
	Let $V$ be a vector space over a field $\FF$ and let $S$ be a nonempty subset of $V$.
	Then $\spann S$ is a subspace of $V$ and is the smallest subspace of $V$ containing $S$.
\end{thm}

\begin{proof}
	Let $\set{s_j}_{j=1}^n\subseteq S$ where $n < \infty$.
	Then $0=\sum_{j=1}^n 0 s_j\in\spann S$.
	Let $x,y\in\spann S$.
	Then there exist $\set{s_j}_{j=1}^n,\set{r_j}_{j=1}^m\subseteq S$ and $\set{a_j}_{j=1}^n,\set{b_j}_{j=1}^m\subseteq\FF$ with $m,n <\infty$ such that $x=\sum_{j=1}^n a_js_j$ and $y=\sum_{j=1}^mb_jr_j$.
	Define $\set{t_j}_{j=1}^{n+m}$ and $\set{c_j}_{j=1}^{n+m}$ by
	\[
		t_j =
		\begin{cases}
			s_j & j\leq n\\
			r_j & j > n
		\end{cases}
		\hspace{1in}
		c_j =
		\begin{cases}
			a_j & j \leq n\\
			b_j & j > n
		\end{cases}.
	\]
	We can see that $\set{t_j}$ is a finite subset of $S$ and $\set{c_j}$ is a finite subset of $\FF$, thus any element made out of scalar multiples of $t$ vectors is in $\spann S$.
	Consider $x+y$.
	\[
		x+y = \sum_{j=1}^n a_js_j + \sum_{j=1}^mb_jr_j
		= \sum_{j=1}^n c_jt_j + \sum_{j=n+1}^{n+m}b_{j-n}r_{j-n}
		= \sum_{j=1}^n c_jt_j + \sum_{j=n+1}^{n+m} c_jt_j
	\]
	Therefore, $x+y$ is a linear combination of elements of $S$ and thus $x+y\in\spann S$.
	Consider $kx$ for any $k\in\FF$.
	\[
		kx=k\sum_{j=1}^n a_js_j=\sum_{j=1}^n (ka_j)s_j
	\]
	Ergo, $kx\in\spann S$ and $\spann S$ is a subspace of $V$.

	Let $W$ be a subspace of $V$ such that $S\subseteq W$.
	Then for all $s\in S$ and $a\in\FF$, $as\in W$ since $W$ is a subspace.
	Ergo, for any $\set{s_j}_{j=1}^n\subseteq S$ and $\set{a_j}_{j=1}^n\subseteq\FF$ with $n < \infty$
	\[
		\sum_{j=1}^n a_js_j\in W
	\]
	since $W$ is a subspace and any finite sum of vectors in $W$ is in $W$.
	Ergo $\spann S\subseteq W$ and is the smallest subspace of $V$ containing $S$.
\end{proof}

\begin{definition}[Span]
A subset $S$ of a vector space $V$ \textit{spans $V$} if and only if $\spann S=V$.
\end{definition}

\begin{proposition}
	Let $W$ be a nonempty subset of a vector space $V$ over a field $\FF$.
	Then $W$ is a subspace of $V$ if and only if $W=\spann W$.
\end{proposition}
\begin{proof}
	$\Rightarrow$ Suppose $W$ is a subspace.
	We know $W\subseteq\spann W$, by definition.
	Furthermore, for all $w\in W$ and $a\in\FF$, $aw\in W$ since $W$ is a subspace.
	Thus, for any finite $\set{w_j}_{j=1}^n\subseteq W$ and $\set{a_j}_{j=1}^n\subseteq\FF$, $\sum a_jw_j\in W$ by properties of vector spaces.
	Ergo, $\spann W\subseteq W$ and $W=\spann W$.

	$\Leftarrow$ Suppose $W=\spann W$.
	Since $\spann W$ is a subspace and $W=\spann W$, $W$ is trivially a subspace.
\end{proof}

\begin{proposition}
	Let $S,R$ be nonempty subsets of $V$ such that $S\subseteq R$.
	Then $\spann S\subseteq \spann R$ and if $\spann S=V$, then $\spann R=V$.
\end{proposition}
\begin{proof}
	I provide a sketch and leave the details to the reader.

	All vectors in $S$ are also in $R$ ergo all sums of scalar multiples of vectors in $S$ (read: $\spann S$) are in $\spann R$.

	Furthermore, we know $\spann R$ is a subspace of $V$, and thus $\spann R\subseteq V$.
	If $V=\spann S\subseteq \spann R$, then $V\subseteq \spann R$ and thus $\spann R=V$.
\end{proof}

\pagebreak

\subsection{Linear Independence}

\begin{definition}[Linear Independence]
	A subset $S$ of a vector space $V$ over a field $\FF$ is \textit{linearly independent} if and only if for any $\set{x_j}_{j=1}^n\subseteq V$ where $n<\infty$ the statement
	\[
		\sum_{j=1}^n a_jx_j=0
	\]
	implies that $\set{a_j}=\set{0}$, where $\set{a_j}\subseteq\FF$.
	Furthermore, if $S$ is not linearly independent, we say that $S$ is \textit{linearly dependent}.
\end{definition}

\begin{example}
	Let $\FF$ be a field and $n\in\NN$.
	Define, $e_j:=(a_1,a_2,\ldots,a_n)\in\FF^n$ where $a_j=1$ and $a_i=0$ for all $i\neq j$.
	Then $\set{e_1,e_2,\ldots,e_n}$ is linearly independent.
\end{example}
\begin{proof}
	Let $\set{c_j}_{j=1}^n\subseteq\FF$ such that $\sum c_je_j=0$.
	Consider $c_je_j$.
	On the $j$th entry of this vector, we will have $c_j$ and all other entries are zero.
	Therefore,
	\[
		\sum c_je_j=(c_1,c_2,\ldots,c_n)=(0,0,\ldots,0).
	\]
	By our definition of vector equality in $\FF^n$ we have $c_j=0$ for all $1\leq j\leq n$.
	Thus, $\set{e_j}_{j=1}^n$ is linearly independent.
\end{proof}

\begin{thm}
	A subset $S$ of a vector space $V$ over a field $\FF$ is linearly dependent if and only if $x_1=0$ or there exists a $k<n$ such that $x_{k+1}\in\spann \set{x_1,x_2,\ldots,x_k}$.
\end{thm}
\begin{proof}
	$\Rightarrow$ Assume $S$ is linearly dependent.
	Therefore, there exists a $\set{a_j}_{j=1}^n\subseteq\FF$ with $\set{a_j}_{j=1}^n\neq\set{0}$ such that $\sum a_jx_j=0$.
	Define $k=\max\set{j|a_j\neq 0}$.
	If $1<k\leq n$, then
	\[
		\sum_{j=1}^n a_jx_j=\sum_{j=1}^k a_jx_j=0
	\]
	and
	\[
		x_k = \sum_{j=1}^n(-a_ja_k^{-1})\in\spann\set{x_1,x_2,\ldots,x_{k-1}}.
	\]
	If $k=1$, then
	\[
		\sum_{j=1}^n a_jx_j=a_1x_1=0
	\]
	with $a_1\neq 0$.
	Ergo, $x_1=0$.

	$\Leftarrow$ Assume $x_1=0$.
	Then $ax_1=0$ for all $a\in\FF$ and $S$ is linearly dependent.
	Assume there exists a $k<n$ such that $x_{k+1}\in\spann \set{x_1,x_2,\ldots, x_k}$.
	Then there exists $\set{a_j}\subseteq\FF$ such that $x_{k+1}=\sum_{j=1}^k a_jx_j$.
	We know $\sum_{j=1}^k a_jx_j - x_{k+1}$ is a linear combination of vectors in $\FF$ and
	\[
		\sum_{j=1}^k a_jx_j - x_{k+1}=0
	\]
	thus, $S$ is linearly dependent.
\end{proof}

\pagebreak

\subsection{Bases and Dimension}

\begin{definition}[Basis of a Vector Space]
	A \textit{basis} $B$ for a vector space $V$ is a a linearly independent subset of $V$ that spans $V$.
\end{definition}

\begin{thm}
	Let $S$ be a linearly independent subset of a vector space $V$ over a field $\FF$ and $x\in V\setminus S$.
	Then, $S\cup\set{x}$ is linearly dependent if and only if $x\in\spann S$.
\end{thm}
