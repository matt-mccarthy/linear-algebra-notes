\section{Vector Spaces}

\subsection{Introduction to Vector Spaces}

\begin{definition}[Vector Space]
	A \textit{vector space} $V$ over a field $\FF$ is a set with two binary operations, $+:V\times V\rightarrow V$ and $\cdot:V\times\FF\rightarrow V$ such that all of the following hold.
	\begin{enumerate}
		\item For all $x,y\in V$, $x+y=y+x$. (Additive Commutativity)
		\item For all $x,y,z\in V$, $x+(y+z)=(x+y)+z$. (Additive Associativity)
		\item There exists an element, denoted 0, in $V$ such that for all $x\in V$, $x+0=x$.
		\item For each $x\in V$ there exists a $y\in V$, denoted $-x$, such that $x+y=0$.
		\item For all $x\in V$, $1x=x$.
		\item For all $a,b\in\FF$ and $x\in V$, $a(bx)=(ab)x$.
		\item For all $a\in\FF$ and $x,y\in V$, $a(x+y)=ax+ay$.
		\item For all $a,b\in\FF$ and $x\in V$, $(a+b)x=ax+bx$.
	\end{enumerate}
	Furthermore, $x+y$ is called the \textit{sum of $x$ and $y$} while $ax$ is called the \textit{product of $x$ and $a$}.
	Moreover, each $x\in V$ is called a \textit{vector} and each $a\in\FF$ is called a \textit{scalar}.
\end{definition}

\begin{definition}[$n$-tuple]
	An object of the form $(a_1,a_2,\ldots,a_n)$ where $a_j\in\FF$ for all $1\leq j\leq n$, is called an \textit{$n$-tuple}.
\end{definition}

\begin{definition}
	Let $\FF$ be a field and $m,n\in\NN$, then an \textit{$m\times n$ matrix} with entries from $\FF$ is a rectangular array of the form
	\[
		A=
		\begin{pmatrix}
			a_{1,1} & a_{1,2} & \ldots & a_{1,n}\\
			a_{2,1} & a_{2,2} & \ldots & a_{2,n}\\
			\vdots & \vdots & \ddots & \vdots\\
			a_{m,1} & a_{m,2} & \ldots & a_{m,n}
		\end{pmatrix}
	\]
	where $a_{i,j}\in\FF$ for all $1\leq i \leq m$ and $1\leq j \leq n$.
	The entries $(a_{i,1},a_{i,2},\ldots,a_{i,n})$ is called the \textit{$i$th row} of the matrix and is a row vector in $\FF^n$.
	The entries $(a_{1,j},a_{2,j},\ldots,a_{m,j})$ is called the \textit{$j$th column} of the matrix and is a column vector in $\FF^n$.
	We denote the entry on the $i$th row and $j$th column as $A_{i,j}$.
	Furthermore, two $m\times n$ matrices, $A$ and $B$, are equal if and only if $A_{i,j}=B_{i,j}$ for all $1\leq i\leq m$ and $1\leq j\leq n$; we denote this by $A=B$.
	Moreover, if $n=m$ we say that $A$ is a \textit{square matrix}.
	Lastly, we denote the set of $m\times n$ matrices over $\FF$ as $M_{m\times n}(\FF)$.
\end{definition}

\begin{definition}[Polynomial Ring]
	Let $\FF$ be a field.
	Then the \textit{ring of polynomials in an indeterminate $x$ over $\FF$}, denoted $\FF[x]$ is defined as
	\[
		\FF[x] := \set{\sum_{i=0}^n a_i x^i | n\in\NN, (a_0,a_1,\ldots,a_n)\in\FF^n, a_n\neq 0}.
	\]
	Additionally, we define $x^0=1$.
	Moreover, for each $p=\sum_{i=0}^n p_ix^i\in\FF[x]$, the \textit{degree of $p$}, denoted $\deg p$, is $n$.
	Furthermore, if $p=0$, that is $p_n=p_{n-1}=\ldots=p_0=0$, then $p$ is called the \textit{zero polynomial} and $\deg p = -1$ or $\deg p =-\infty$ depending on convention.
	If $\deg p=0$, then we say $p$ is a \textit{constant polynomial}.
	Lastly, $\FF[x]$ forms a ring under the following operations where $p,q\in\FF[x]$ and without loss of generality assume, $\deg p \geq\deg q$.
	\begin{align*}
		p+q &=\sum_{i=0}^{\deg q} (p_i+q_i)x^i +\sum_{i=\deg q + 1}^{\deg p} p_ix^i\\
		p q &= \sum_{k=0}^{\deg p \cdot \deg q} \paren{\sum_{i+j=k} p_iq_j} x^k
	\end{align*}
\end{definition}

\pagebreak

\subsection{Subspaces}

\begin{definition}[Subspace]
	A \textit{subspace}, $W$, of a vector space, $V$, over a field, $\FF$, is a subset of $V$ that is also a vector space over $\FF$.
\end{definition}

\begin{definition}[Matrix Transpose]
	Let $M$ be an $m\times n$ matrix, then the \textit{transpose of $M$}, denoted $M^T$, is the $n\times m$ matrix defined by $(M^T)_{i,j}=M_{j,i}$, that is
	\[
		M^T=
		\begin{pmatrix}
			M_{1,1} & M_{2,1} & \ldots & M_{m,1}\\
			M_{1,2} & M_{2,2} & \ldots & M_{m,2}\\
			\vdots & \vdots & \ddots & \vdots\\
			M_{1,n} & M_{2,n} & \ldots & M_{m,n}
		\end{pmatrix}.
	\]
\end{definition}

\begin{definition}[Symmetric Matrix]
	Let $M$ be a matrix, then if $M=M^T$, we say $M$ is \textit{symmetric}.
\end{definition}

\begin{definition}[Main Diagonal of a Matrix]
	Let $\FF$ be a field and let $M\in M_{n\times n}(\FF)$, then the \textit{main diagonal of $M$} is the set $\set{M_{i,i}}_{i=1}^n$.
\end{definition}

\begin{definition}[Diagonal Matrix]
	Let $\FF$ be a field and let $A\in M_{n\times n}(\FF)$, then $A$ is called a \textit{diagonal matrix} if and only if whenever $i\neq j$, $A_{i,j}=0$.
\end{definition}

\begin{definition}[Trace of a Matrix]
	Let $\KK$ be a field and let $M\in M_{n\times n}(\KK)$, then the \textit{trace of $M$} denoted $\trace M$ is defined as
	\[
		\trace M = \sum_{i=1}^n M_{i,i}
	\]
	or the sum of the elements on the main diagonal.
\end{definition}

\begin{definition}[Sum of Subsets of a Vector Space]
	Let $S,R$ be nonempty subsets of a vector space $V$, then the \textit{sum of $S$ and $R$}, denoted $S+R$ is defined as $S+R=\set{s+r|s\in S, r\in R}$.
\end{definition}

\begin{definition}[Direct Sum of Vector Spaces]
	A vector space $V$ is called the \textit{direct sum of $U$ and $W$}, denoted $V=U\oplus W$ if and only if $U$ and $W$ are subspaces of $V$ such that $U\cap W=\emptyset$ and $U+W=V$.
\end{definition}

\begin{definition}[Cosets of a Vector Space]
	Let $U$ be a subspace of a vector space $V$ over a field $\KK$.
	Then for each $v\in V$ the set $\set{v}+W=\set{v+w}_{w\in W}$ is called the \textit{coset of $W$ containing $v$}, denoted $v+W$.
\end{definition}

\begin{definition}[Quotient Space]
	Let $\KK$ be a field and $W$ be a subspace of a vector space $V$ over $\FF$.
	The the \textit{quotient space of $V$ modulo $W$}, denoted $V/W$ is the set of all cosets of $W$,
	\[
		V/W := \set{v+W}_{v\in V}.
	\]
	Furthermore $V/W$ is a vector space under the following operations.
	\begin{align*}
		(u+W)+(v+W)&=(u+v)+W\\
		a(u+W)&=(au)+W
	\end{align*}
\end{definition}

\pagebreak

\subsection{Linear Combinations}

\begin{definition}[Linear Combination]
	Let $V$ be a vector space over a field $\FF$ and let $S$ be a nonempty subset of $V$.
	An $x\in V$ is said to be a \textit{linear combination of elements of $S$} if and only if there exists a $\set{s_j}_{j=1}^n\subseteq S$ and scalars $\set{a_j}_{j=1}^n\subseteq\FF$ where $n < \infty$ such that
	\[
		x=\sum_{j=1}^n a_j y_j.
	\]
	When this happens, we say $x$ is a \textit{linear combination of $y_1,y_2,\ldots, y_n$}.
\end{definition}

\begin{definition}[Spanning Set]
	Let $V$ be a vector space over a field $\FF$ and let $S$ be a nonempty subset of $V$.
	Then, the \textit{span of $S$}, denoted $\spann S$, is the set
	\[
		\spann S = \set{\sum_{j=1}^n a_j s_j | \set{a_j}_{j=1}^n\subseteq\FF, \set{s_j}_{j=1}^n\subseteq S, n<\infty}
	\]
	or the set of linear combinations of elements of $S$.
	We define $\spann\emptyset = \set{0}$.
\end{definition}

\begin{definition}[Span]
A subset $S$ of a vector space $V$ \textit{spans $V$} if and only if $\spann S=V$.
\end{definition}

\pagebreak

\subsection{Linear Independence}

\begin{definition}[Linear Independence]
	A subset $S$ of a vector space $V$ over a field $\FF$ is \textit{linearly independent} if and only if for any $\set{x_j}_{j=1}^n\subseteq V$ where $n<\infty$ the statement
	\[
		\sum_{j=1}^n a_jx_j=0
	\]
	implies that $\set{a_j}=\set{0}$, where $\set{a_j}\subseteq\FF$.
	Furthermore, if $S$ is not linearly independent, we say that $S$ is \textit{linearly dependent}.
\end{definition}

\begin{definition}[Basis of a Vector Space]
	A \textit{basis} $B$ for a vector space $V$ is a a linearly independent subset of $V$ that spans $V$.
\end{definition}
